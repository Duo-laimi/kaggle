{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fa5a6c1362dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    dtype = dtype, # None for auto detection\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3a7bf7-6335-474a-8f30-7bebd682d35d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T06:58:04.578624Z",
     "iopub.status.busy": "2026-01-31T06:58:04.578414Z",
     "iopub.status.idle": "2026-01-31T07:01:45.262368Z",
     "shell.execute_reply": "2026-01-31T07:01:45.261874Z",
     "shell.execute_reply.started": "2026-01-31T06:58:04.578601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:32<00:00, 42.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = \"/mnt/workspace/model/gpt-oss-20b\"\n",
    "model_path = \"/mnt/workspace/model/qwen3-8b\"\n",
    "# gguf_file = \"/mnt/workspace/model/gpt-oss-20b-gguf/gpt-oss-20b-Q4_K_M.gguf\"\n",
    "\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f2bed1-cb31-4b9c-890e-79220182c43c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T07:04:54.276238Z",
     "iopub.status.busy": "2026-01-31T07:04:54.276077Z",
     "iopub.status.idle": "2026-01-31T07:04:54.280094Z",
     "shell.execute_reply": "2026-01-31T07:04:54.279538Z",
     "shell.execute_reply.started": "2026-01-31T07:04:54.276223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a good math problem solver. Give the final answer in 'Answer: \\boxed{final_answer}' format.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "what is 32 \\times 48? \n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question1 = \\\n",
    "\"\"\"\n",
    "Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ‚ÄúIf\n",
    "we each added the number of sweets we‚Äôre holding to our (positive integer) age, my answer would\n",
    "be double yours. If we took the product, then my answer would be four times yours.‚Äù Bob replies:\n",
    "‚ÄúWhy don‚Äôt you give me five of your sweets because then both our sum and product would be equal.‚Äù\n",
    "What is the product of Alice and Bob‚Äôs ages?\n",
    "\"\"\"\n",
    "\n",
    "question = \\\n",
    "\"\"\"\n",
    "what is 32 \\\\times 48? \n",
    "\"\"\"\n",
    "# Think step by step  and g\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a good math problem solver. Give the final answer in 'Answer: \\\\boxed{final_answer}' format.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    enable_thinking=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa36951d-86c4-43d3-8f44-2ee7101561b6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T07:01:58.407493Z",
     "iopub.status.busy": "2026-01-31T07:01:58.407319Z",
     "iopub.status.idle": "2026-01-31T07:02:07.765680Z",
     "shell.execute_reply": "2026-01-31T07:02:07.765217Z",
     "shell.execute_reply.started": "2026-01-31T07:01:58.407477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|im_start|>system\n",
      "You are a good math problem solver. Think step by step  and give the final answer in 'Answer: \\boxed{final_answer}' format.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "what is 32 \\times 48? \n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "Response: To solve $ 32 \\times 48 $, we can use the standard multiplication algorithm or break it down for easier computation.\n",
      "\n",
      "### Step-by-step breakdown:\n",
      "\n",
      "We can use the **distributive property**:\n",
      "\n",
      "$$\n",
      "32 \\times 48 = 32 \\times (50 - 2) = 32 \\times 50 - 32 \\times 2\n",
      "$$\n",
      "\n",
      "Now compute each part:\n",
      "\n",
      "- $ 32 \\times 50 = 1600 $\n",
      "- $ 32 \\times 2 = 64 $\n",
      "\n",
      "Now subtract:\n",
      "\n",
      "$$\n",
      "1600 - 64 = 1536\n",
      "$$\n",
      "\n",
      "### Final Answer:\n",
      "$$\n",
      "\\boxed{1536}\n",
      "$$"
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "# skip_special_tokens=True ËøáÊª§Êéâ <|endoftext|> Á≠âÁâπÊÆäÂ≠óÁ¨¶\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 3. Â∞ÅË£ÖÁîüÊàêÂáΩÊï∞\n",
    "def stream_chat(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Â∞ÜÁîüÊàêËøáÁ®ãÊîæÂÖ•ÂêéÂè∞Á∫øÁ®ã\n",
    "    generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=4096)\n",
    "    thread = Thread(target=chat_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Âú®‰∏ªÁ∫øÁ®ã‰∏≠ÂÆûÊó∂ÊâìÂç∞\n",
    "    print(f\"Prompt: {prompt}\\nResponse: \", end=\"\")\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    \n",
    "    thread.join()\n",
    "\n",
    "# 4. ËøêË°å\n",
    "stream_chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4ba8de-d5a4-4dc7-9750-108519dda89f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T07:16:13.093389Z",
     "iopub.status.busy": "2026-01-31T07:16:13.093221Z",
     "iopub.status.idle": "2026-01-31T07:16:21.943899Z",
     "shell.execute_reply": "2026-01-31T07:16:21.943375Z",
     "shell.execute_reply.started": "2026-01-31T07:16:13.093374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = chat_model.generate(**inputs, streamer=streamer, max_new_tokens=2048)\n",
    "outputs = chat_model.generate(**inputs, max_new_tokens=2048)\n",
    "text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5542c542-6116-4f7d-89e2-28a18d271463",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T07:20:47.227727Z",
     "iopub.status.busy": "2026-01-31T07:20:47.227564Z",
     "iopub.status.idle": "2026-01-31T07:20:47.231714Z",
     "shell.execute_reply": "2026-01-31T07:20:47.231046Z",
     "shell.execute_reply.started": "2026-01-31T07:20:47.227713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_answer(text):\n",
    "    # 1. ÁßªÈô§ÊÄùËÄÉËøáÁ®ãÔºåËé∑ÂèñËæìÂá∫ÈÉ®ÂàÜ\n",
    "    if \"</think>\" in text:\n",
    "        _, output = text.split(\"</think>\", 1)\n",
    "    else:\n",
    "        output = text\n",
    "\n",
    "    # 2. ÂåπÈÖç \\boxed{ÂÜÖÂÆπ} ‰∏≠ÁöÑÂÜÖÂÆπ\n",
    "    # ‰ΩøÁî®ÊçïËé∑ÁªÑ () Êù•ÊèêÂèñÊï∞Â≠óÔºåÂπ∂Â§ÑÁêÜÂèØËÉΩÂ≠òÂú®ÁöÑÁ©∫Ê†º\n",
    "    pattern = r\"\\\\boxed{([^{}]+)}\"\n",
    "    matches = re.findall(pattern, output)\n",
    "    # 3. ËøîÂõûÊúÄÂêé‰∏Ä‰∏™ÂåπÈÖçÈ°π\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return None\n",
    "\n",
    "answer = extract_answer(text)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2ada65-b3a1-4f4b-83d9-8029f89e0904",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-31T08:00:51.202092Z",
     "iopub.status.busy": "2026-01-31T08:00:51.201923Z",
     "iopub.status.idle": "2026-01-31T08:00:55.037380Z",
     "shell.execute_reply": "2026-01-31T08:00:55.036712Z",
     "shell.execute_reply.started": "2026-01-31T08:00:51.202076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.base import KaggleSolver\n",
    "model_path = \"/mnt/workspace/model/qwen3-8b\"\n",
    "max_seq_length = 2048\n",
    "solver = KaggleSolver(model_path, max_seq_length=max_seq_length, inference_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deade2d4-5f3e-4dbd-82d2-73794488d2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T08:00:57.507218Z",
     "iopub.status.busy": "2026-01-31T08:00:57.506908Z",
     "iopub.status.idle": "2026-01-31T08:02:38.143436Z",
     "shell.execute_reply": "2026-01-31T08:02:38.142727Z",
     "shell.execute_reply.started": "2026-01-31T08:00:57.507200Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load model from /mnt/workspace/model/qwen3-8b.\n",
      "Formatted Prompt: \n",
      "<|im_start|>system\n",
      "You are a good math problem solver. Give the final answer in 'Answer:oxed{final_answer}' format.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "\n",
      "Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ‚ÄúIf\n",
      "we each added the number of sweets we‚Äôre holding to our (positive integer) age, my answer would\n",
      "be double yours. If we took the product, then my answer would be four times yours.‚Äù Bob replies:\n",
      "‚ÄúWhy don‚Äôt you give me five of your sweets because then both our sum and product would be equal.‚Äù\n",
      "What is the product of Alice and Bob‚Äôs ages?\n",
      " \n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "Generate Output: \n",
      "\n",
      "\n",
      "We are given a word problem involving Alice and Bob, who each have some integer number of sweets and some integer age. Let's define:\n",
      "\n",
      "- Let $ a $ be Alice's age.\n",
      "- Let $ b $ be Bob's age.\n",
      "- Let $ s_A $ be the number of sweets Alice has.\n",
      "- Let $ s_B $ be the number of sweets Bob has.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Translate Alice's statements into equations\n",
      "\n",
      "**First statement:**\n",
      "\n",
      "> \"If we each added the number of sweets we‚Äôre holding to our (positive integer) age, my answer would be double yours.\"\n",
      "\n",
      "This means:\n",
      "$$\n",
      "a + s_A = 2(b + s_B)\n",
      "$$\n",
      "\n",
      "**Second statement:**\n",
      "\n",
      "> \"If we took the product, then my answer would be four times yours.\"\n",
      "\n",
      "This means:\n",
      "$$\n",
      "(a + s_A)(b + s_B) = 4(b + s_B)\n",
      "$$\n",
      "\n",
      "Let‚Äôs simplify this second equation:\n",
      "$$\n",
      "(a + s_A)(b + s_B) = 4(b + s_B)\n",
      "$$\n",
      "Divide both sides by $ b + s_B $ (assuming $ b + s_B \\ne 0 $):\n",
      "$$\n",
      "a + s_A = 4\n",
      "$$\n",
      "\n",
      "So now we have two equations:\n",
      "\n",
      "1. $ a + s_A = 2(b + s_B) $\n",
      "2. $ a + s_A = 4 $\n",
      "\n",
      "From equation (2), we get:\n",
      "$$\n",
      "a + s_A = 4 \\quad \\text{(Equation A)}\n",
      "$$\n",
      "\n",
      "Substitute into Equation (1):\n",
      "$$\n",
      "4 = 2(b + s_B) \\Rightarrow b + s_B = 2 \\quad \\text{(Equation B)}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Use Bob's statement\n",
      "\n",
      "> \"Why don‚Äôt you give me five of your sweets because then both our sum and product would be equal.\"\n",
      "\n",
      "This means, if Alice gives 5 sweets to Bob:\n",
      "\n",
      "- Alice now has $ s_A - 5 $ sweets\n",
      "- Bob now has $ s_B + 5 $ sweets\n",
      "\n",
      "Then:\n",
      "$$\n",
      "(s_A - 5) + (s_B + 5) = (s_A + s_B)\n",
      "$$\n",
      "So the **sum** of their new ages and sweets is:\n",
      "$$\n",
      "(a + s_A - 5) + (b + s_B + 5) = a + s_A + b + s_B = (a + s_A) + (b + s_B)\n",
      "$$\n",
      "This is the same as before, so the **sum** remains the same. That‚Äôs not useful.\n",
      "\n",
      "But the **product** becomes equal:\n",
      "$$\n",
      "(a + s_A - 5)(b + s_B + 5) = (a + s_A)(b + s_B)\n",
      "$$\n",
      "\n",
      "Let‚Äôs use our earlier values:\n",
      "- From Equation A: $ a + s_A = 4 $\n",
      "- From Equation B: $ b + s_B = 2 $\n",
      "\n",
      "So:\n",
      "$$\n",
      "(4 - 5)(2 + 5) = (4)(2)\n",
      "$$\n",
      "$$\n",
      "(-1)(7) = 8 \\Rightarrow -7 = 8 \\quad \\text{Contradiction!}\n",
      "$$\n",
      "\n",
      "So our assumption that $ a + s_A = 4 $ and $ b + s_B = 2 $ is **not consistent** with Bob's statement. So we must re-evaluate.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Re-examine the problem carefully\n",
      "\n",
      "Let‚Äôs denote:\n",
      "- $ a + s_A = x $\n",
      "- $ b + s_B = y $\n",
      "\n",
      "Then from Alice's statements:\n",
      "- $ x = 2y $\n",
      "- $ x \\cdot y = 4y $\n",
      "\n",
      "From $ x \\cdot y = 4y $, we get:\n",
      "$$\n",
      "x = 4 \\quad \\text{(since } y \\ne 0\\text{)}\n",
      "$$\n",
      "\n",
      "So:\n",
      "- $ x = 4 $\n",
      "- $ y = 2 $\n",
      "\n",
      "So:\n",
      "- $ a + s_A = 4 $\n",
      "- $ b + s_B = 2 $\n",
      "\n",
      "Now, from Bob's statement:\n",
      "> \"Why don‚Äôt you give me five of your sweets because then both our sum and product would be equal.\"\n",
      "\n",
      "That is, after Alice gives 5 sweets to Bob:\n",
      "\n",
      "- Alice has $ s_A - 5 $ sweets\n",
      "- Bob has $ s_B + 5 $ sweets\n",
      "\n",
      "So their **new** ages and sweets:\n",
      "- Alice: $ a + s_A - 5 = 4 - 5 = -1 $\n",
      "- Bob: $ b + s_B + 5 = 2 + 5 = 7 $\n",
      "\n",
      "So their **new** values are:\n",
      "- Alice: -1\n",
      "- Bob: 7\n",
      "\n",
      "So the **product** of their **new** values is:\n",
      "$$\n",
      "(-1)(7) = -7\n",
      "$$\n",
      "And the **product** of their **original** values was:\n",
      "$$\n",
      "(4)(2) = 8\n",
      "$$\n",
      "\n",
      "So the **product of their original values** (i.e., their ages) is:\n",
      "$$\n",
      "a \\cdot b\n",
      "$$\n",
      "\n",
      "We are asked: **What is the product of Alice and Bob‚Äôs ages?**\n",
      "\n",
      "We already have:\n",
      "- $ a + s_A = 4 $\n",
      "- $ b + s_B = 2 $\n",
      "\n",
      "So:\n",
      "- $ s_A = 4 - a $\n",
      "- $ s_B = 2 - b $\n",
      "\n",
      "From Bob's statement, we have:\n",
      "$$\n",
      "(a + s_A - 5)(b + s_B + 5) = (a + s_A)(b + s_B)\n",
      "$$\n",
      "\n",
      "Substitute:\n",
      "$$\n",
      "(4 - 5)(2 + 5) = (4)(2)\n",
      "\\Rightarrow (-1)(7) = 8 \\Rightarrow -7 = 8\n",
      "$$\n",
      "\n",
      "This contradiction suggests we must consider that the values of $ a $ and $ b $ are such that:\n",
      "$$\n",
      "(a + s_A)(b + s_B) = 8 \\quad \\text{and} \\quad a + s_A = 4, \\quad b + s_B = 2\n",
      "$$\n",
      "So:\n",
      "$$\n",
      "(4)(2) = 8\n",
      "$$\n",
      "\n",
      "So this is consistent.\n",
      "\n",
      "Now, from:\n",
      "- $ a + s_A = 4 $\n",
      "- $ b + s_B = 2 $\n",
      "\n",
      "We want to find $ a \\cdot b $\n",
      "\n",
      "We also know:\n",
      "- $ s_A = 4 - a $\n",
      "- $ s_B = 2 - b $\n",
      "\n",
      "So the number of sweets each has must be **positive integers**, so:\n",
      "- $ s_A = 4 - a > 0 \\Rightarrow a < 4 $\n",
      "- $ s_B = 2 - b > 0 \\Rightarrow b < 2 $\n",
      "\n",
      "So $ a < 4 $, $ b < 2 $\n",
      "\n",
      "Try integer values for $ a $ and $ b $ such that:\n",
      "- $ a + s_A = 4 $\n",
      "- $ b + s_B = 2 $\n",
      "- $ s_A = 4 - a > 0 $\n",
      "- $ s_B = 2 - b > 0 $\n",
      "\n",
      "Try:\n",
      "- $ a = 3 $, $ s_A = 1 $\n",
      "- $ b = 1 $, $ s_B = 1 $\n",
      "\n",
      "Then:\n",
      "- $ a \\cdot b = 3 \\cdot 1 = 3 $\n",
      "\n",
      "Check:\n",
      "- $ a + s_A = 3 + 1 = 4 $\n",
      "- $ b + s_B = 1 + 1 = 2 $\n",
      "- $ s_A = 1 $, $ s_B = 1 $ ‚Üí both positive integers\n",
      "\n",
      "So this satisfies all conditions.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{3}\n",
      "$$<|im_end|>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "problem = \\\n",
    "\"\"\"\n",
    "Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ‚ÄúIf\n",
    "we each added the number of sweets we‚Äôre holding to our (positive integer) age, my answer would\n",
    "be double yours. If we took the product, then my answer would be four times yours.‚Äù Bob replies:\n",
    "‚ÄúWhy don‚Äôt you give me five of your sweets because then both our sum and product would be equal.‚Äù\n",
    "What is the product of Alice and Bob‚Äôs ages?\n",
    "\"\"\"\n",
    "answer = solver.predict(problem)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b0ea04-60c6-436e-8a49-ba74b026e141",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T04:38:08.715585Z",
     "iopub.status.busy": "2026-01-28T04:38:08.715394Z",
     "iopub.status.idle": "2026-01-28T04:38:11.829440Z",
     "shell.execute_reply": "2026-01-28T04:38:11.828795Z",
     "shell.execute_reply.started": "2026-01-28T04:38:08.715570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12923/2014739007.py:1: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/usr/local/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEiPKcS2_ib",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m1024\u001b[39m\n\u001b[32m      4\u001b[39m dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/__init__.py:279\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# TODO: check triton for intel installed properly.\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:75\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastBaseModel\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Final patching code\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     79\u001b[39m     LlamaAttention,\n\u001b[32m     80\u001b[39m     LlamaDecoderLayer,\n\u001b[32m     81\u001b[39m     LlamaModel,\n\u001b[32m     82\u001b[39m     LlamaForCausalLM,\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/vision.py:127\u001b[39m\n\u001b[32m    120\u001b[39m _compile_config = CompileConfig(\n\u001b[32m    121\u001b[39m     fullgraph = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    122\u001b[39m     dynamic = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    123\u001b[39m     mode = \u001b[33m\"\u001b[39m\u001b[33mreduce-overhead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    125\u001b[39m _compile_config.disable = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Must set manually\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    128\u001b[39m     convert_lora_modules,\n\u001b[32m    129\u001b[39m     return_lora_modules,\n\u001b[32m    130\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    133\u001b[39m     torch_compiler_set_stance = torch.compiler.set_stance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth_zoo/vllm_utils.py:155\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Patch apply_bnb_4bit\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbitsandbytes\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    157\u001b[39m     vllm.model_executor.layers.quantization.bitsandbytes,\n\u001b[32m    158\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapply_bnb_4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m ):\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Fix force using torch.bfloat16 all the time and make it dynamic\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_4bit_weight\u001b[39m(\n\u001b[32m    162\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    163\u001b[39m         layer: torch.nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m     ) -> torch.Tensor:\n\u001b[32m    167\u001b[39m         \u001b[38;5;66;03m# only load the bitsandbytes module when needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/model_executor/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasevLLMParameter, PackedvLLMParameter\n\u001b[32m      6\u001b[39m __all__ = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBasevLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPackedvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/model_executor/parameter.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     get_tensor_model_parallel_rank,\n\u001b[32m     13\u001b[39m     get_tensor_model_parallel_world_size,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     17\u001b[39m __all__ = [\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBasevLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPackedvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRowvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommunication_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/communication_op.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtensor_model_parallel_all_reduce\u001b[39m(input_: torch.Tensor) -> torch.Tensor:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/parallel_state.py:250\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.ops.symm_mem.fused_scaled_matmul_reduce_scatter(\n\u001b[32m    233\u001b[39m         A,\n\u001b[32m    234\u001b[39m         B,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         use_fast_accum,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m supports_custom_op():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[43mdirect_register_custom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall_reduce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_reduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_reduce_fake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     direct_register_custom_op(\n\u001b[32m    257\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mreduce_scatter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    258\u001b[39m         op_func=reduce_scatter,\n\u001b[32m    259\u001b[39m         fake_impl=reduce_scatter_fake,\n\u001b[32m    260\u001b[39m     )\n\u001b[32m    262\u001b[39m     direct_register_custom_op(\n\u001b[32m    263\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mall_gather\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         op_func=all_gather,\n\u001b[32m    265\u001b[39m         fake_impl=all_gather_fake,\n\u001b[32m    266\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/utils/torch_utils.py:638\u001b[39m, in \u001b[36mdirect_register_custom_op\u001b[39m\u001b[34m(op_name, op_func, mutates_args, fake_impl, target_lib, dispatch_key, tags)\u001b[39m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [weak_ref_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(weak_ref_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n\u001b[32m    640\u001b[39m \u001b[38;5;66;03m# For IntermediateTensors used in pipeline parallelism\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/platforms/__init__.py:257\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    256\u001b[39m     platform_cls_qualname = resolve_current_platform_cls_qualname()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     _current_platform = \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[32m    259\u001b[39m     _init_trace = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(traceback.format_stack())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/utils/import_utils.py:122\u001b[39m, in \u001b[36mresolve_obj_by_qualname\u001b[39m\u001b[34m(qualname)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03mResolve an object by its fully-qualified class name.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m module_name, obj_name = qualname.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/platforms/cuda.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m import_pynvml\n",
      "\u001b[31mImportError\u001b[39m: /usr/local/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEiPKcS2_ib"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "model_path = \"/mnt/workspace/model/gpt-oss-20b\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    dtype = dtype,\n",
    "    max_seq_length = max_seq_length, \n",
    "    load_in_4bit = True,  \n",
    "    full_finetuning = False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78145166232e564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41001ce28aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"A dummy model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Simulate model loading.\"\"\"\n",
    "        print(\"Loading model...\")\n",
    "        # Just return a \"model\" that always answers with 0\n",
    "        return lambda problem: 0\n",
    "\n",
    "    def predict(self, problem: str):\n",
    "        # Employ lazy loading: load model on the first model.predict call\n",
    "        if self._model is None:\n",
    "            self._model = self.load()\n",
    "        return self._model(problem)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 99999, inclusive.\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values\n",
    "    id_ = id_.item(0)\n",
    "    problem_text: str = problem.item(0)\n",
    "    # Make a prediction\n",
    "    # The model is loaded on the first call\n",
    "    prediction = model.predict(problem_text)\n",
    "    return pl.DataFrame({'id': id_, 'answer': prediction})\n",
    "\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # You MUST call this within 15 minutes of the script starting. This is to\n",
    "    # ensure a \"fast fail\" in case a bug prevents the inference server from starting.\n",
    "    # Do anything that might take a long time (like model loading) in the predict\n",
    "    # function, which has no time limit.\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
