{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fa5a6c1362dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    dtype = dtype, # None for auto detection\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3a7bf7-6335-474a-8f30-7bebd682d35d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-30T04:23:33.187088Z",
     "iopub.status.busy": "2026-01-30T04:23:33.186914Z",
     "iopub.status.idle": "2026-01-30T04:24:36.396184Z",
     "shell.execute_reply": "2026-01-30T04:24:36.395613Z",
     "shell.execute_reply.started": "2026-01-30T04:23:33.187072Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:55<00:00, 11.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = \"/mnt/workspace/model/gpt-oss-20b\"\n",
    "model_path = \"/mnt/workspace/model/qwen3-8b\"\n",
    "# gguf_file = \"/mnt/workspace/model/gpt-oss-20b-gguf/gpt-oss-20b-Q4_K_M.gguf\"\n",
    "\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f2bed1-cb31-4b9c-890e-79220182c43c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-30T04:46:39.611923Z",
     "iopub.status.busy": "2026-01-30T04:46:39.611679Z",
     "iopub.status.idle": "2026-01-30T04:46:39.616401Z",
     "shell.execute_reply": "2026-01-30T04:46:39.615592Z",
     "shell.execute_reply.started": "2026-01-30T04:46:39.611892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a good math problem solver. Think step by step  and give the final answer in 'Answer: \\boxed{final_answer}' format.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "what is 32 \\times 48? \n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question1 = \\\n",
    "\"\"\"\n",
    "Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ‚ÄúIf\n",
    "we each added the number of sweets we‚Äôre holding to our (positive integer) age, my answer would\n",
    "be double yours. If we took the product, then my answer would be four times yours.‚Äù Bob replies:\n",
    "‚ÄúWhy don‚Äôt you give me five of your sweets because then both our sum and product would be equal.‚Äù\n",
    "What is the product of Alice and Bob‚Äôs ages?\n",
    "\"\"\"\n",
    "\n",
    "question = \\\n",
    "\"\"\"\n",
    "what is 32 \\\\times 48? \n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a good math problem solver. Think step by step  and give the final answer in 'Answer: \\\\boxed{final_answer}' format.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    enable_thinking=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36951d-86c4-43d3-8f44-2ee7101561b6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-30T04:46:43.949636Z",
     "iopub.status.busy": "2026-01-30T04:46:43.949479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|im_start|>system\n",
      "You are a good math problem solver. Think step by step  and give the final answer in 'Answer: \\boxed{final_answer}' format.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "what is 32 \\times 48? \n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Response: <think>\n",
      "Okay, so I need to calculate 32 multiplied by 48. Let me think about how to approach this. Hmm, maybe I can break it down into simpler parts. Let me recall some multiplication strategies.\n",
      "\n",
      "First, I know that multiplying two numbers can sometimes be made easier by using distributive property or breaking them into smaller numbers. Let me try that. Let's see, 32 times 48. Maybe I can think of 48 as 40 + 8. So then, 32 times 40 plus 32 times 8. Let me check that.\n",
      "\n",
      "So, 32 * 40. Well, 32 * 4 is 128, so 32 * 40 would be 128 * 10, which is 1280. Then 32 * 8. Hmm, 32 * 8. Let me calculate that. 30*8 is 240 and 2*8 is 16, so 240 + 16 = 256. So then adding those two results together: 1280 + 256. Let me add those. 1280 + 200 is 1480, then +56 is 1536. So is that the answer? Wait, let me verify.\n",
      "\n",
      "Alternatively, maybe I can use another method to cross-check. Let me try breaking down 32 as 30 + 2. So (30 + 2) * 48. That would be 30*48 + 2*48. Let's compute each part. 30*48. Well, 30*40 is 1200 and 30*8 is 240, so total is 1200 + 240 = 1440. Then 2*48 is 96. Adding those together: 1440 + 96. Let's do that. 1440 + 90 is 1530, plus 6 is 1536. Same result. Okay, so that's consistent.\n",
      "\n",
      "Another way to check: maybe using standard multiplication algorithm. Let me write it out:\n",
      "\n",
      "   32\n",
      "x 48\n",
      "------\n",
      "First, multiply 32 by 8. 2*8=16, carry over 1. 3*8=24, plus 1 is 25. So that's 256. Then multiply 32 by 40 (since 4 is in the tens place, so 4*10=40). So 32*40. Let's do that. 32*4=128, then add a zero at the end, making it 1280. Then add 256 and 1280. Which again is 1280 + 256 = 1536. So same answer.\n",
      "\n",
      "Alternatively, maybe I can use the fact that 32*48 is equal to (32*2)*(48/2) ? Wait, no, that might not help. Wait, maybe not. Alternatively, think of 32*48 as 32*(50 - 2) = 32*50 - 32*2. Let me compute that. 32*50 is 1600 (since 32*5=160, times 10 is 1600). Then subtract 32*2=64. So 1600 - 64 = 1536. Yep, same answer again. So that's another way to get 1536.\n",
      "\n",
      "Hmm, all methods are leading me to 1536. Let me check with another approach. Maybe using the standard multiplication step-by-step:\n",
      "\n",
      "Multiply 32 by 48:\n",
      "\n",
      "First, multiply 32 by 8 (units place): 32*8=256. Write down 56 and carry over 2. Wait, no, actually, when doing standard multiplication, you write down the partial products. Let me think again.\n",
      "\n",
      "Actually, when multiplying 32 by 48, you can do:\n",
      "\n",
      "First, multiply 32 by 8 (the units digit of 48). So 32*8=256. Then multiply 32 by 4 (the tens digit of 48, which is actually 40), so 32*40=1280. Then add them together: 256 + 1280 = 1536. Yep, same result.\n",
      "\n",
      "Alternatively, maybe I can use calculator to verify, but since I can't use a calculator, I need to rely on my calculations. But since all methods give me 1536, I think that's the correct answer.\n",
      "\n",
      "Wait, let me think again. Let me try multiplying 32*48 "
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "# skip_special_tokens=True ËøáÊª§Êéâ <|endoftext|> Á≠âÁâπÊÆäÂ≠óÁ¨¶\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 3. Â∞ÅË£ÖÁîüÊàêÂáΩÊï∞\n",
    "def stream_chat(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Â∞ÜÁîüÊàêËøáÁ®ãÊîæÂÖ•ÂêéÂè∞Á∫øÁ®ã\n",
    "    generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=4096)\n",
    "    thread = Thread(target=chat_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Âú®‰∏ªÁ∫øÁ®ã‰∏≠ÂÆûÊó∂ÊâìÂç∞\n",
    "    print(f\"Prompt: {prompt}\\nResponse: \", end=\"\")\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    \n",
    "    thread.join()\n",
    "\n",
    "# 4. ËøêË°å\n",
    "stream_chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ba8de-d5a4-4dc7-9750-108519dda89f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = chat_model.generate(**inputs, streamer=streamer, max_new_tokens=2048)\n",
    "# print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b0ea04-60c6-436e-8a49-ba74b026e141",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2026-01-28T04:38:08.715585Z",
     "iopub.status.busy": "2026-01-28T04:38:08.715394Z",
     "iopub.status.idle": "2026-01-28T04:38:11.829440Z",
     "shell.execute_reply": "2026-01-28T04:38:11.828795Z",
     "shell.execute_reply.started": "2026-01-28T04:38:08.715570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12923/2014739007.py:1: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/usr/local/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEiPKcS2_ib",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m1024\u001b[39m\n\u001b[32m      4\u001b[39m dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/__init__.py:279\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# TODO: check triton for intel installed properly.\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:75\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastBaseModel\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Final patching code\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     79\u001b[39m     LlamaAttention,\n\u001b[32m     80\u001b[39m     LlamaDecoderLayer,\n\u001b[32m     81\u001b[39m     LlamaModel,\n\u001b[32m     82\u001b[39m     LlamaForCausalLM,\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth/models/vision.py:127\u001b[39m\n\u001b[32m    120\u001b[39m _compile_config = CompileConfig(\n\u001b[32m    121\u001b[39m     fullgraph = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    122\u001b[39m     dynamic = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    123\u001b[39m     mode = \u001b[33m\"\u001b[39m\u001b[33mreduce-overhead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    125\u001b[39m _compile_config.disable = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Must set manually\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    128\u001b[39m     convert_lora_modules,\n\u001b[32m    129\u001b[39m     return_lora_modules,\n\u001b[32m    130\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    133\u001b[39m     torch_compiler_set_stance = torch.compiler.set_stance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unsloth_zoo/vllm_utils.py:155\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Patch apply_bnb_4bit\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbitsandbytes\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    157\u001b[39m     vllm.model_executor.layers.quantization.bitsandbytes,\n\u001b[32m    158\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapply_bnb_4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m ):\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Fix force using torch.bfloat16 all the time and make it dynamic\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_4bit_weight\u001b[39m(\n\u001b[32m    162\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    163\u001b[39m         layer: torch.nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m     ) -> torch.Tensor:\n\u001b[32m    167\u001b[39m         \u001b[38;5;66;03m# only load the bitsandbytes module when needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/model_executor/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasevLLMParameter, PackedvLLMParameter\n\u001b[32m      6\u001b[39m __all__ = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBasevLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPackedvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/model_executor/parameter.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     get_tensor_model_parallel_rank,\n\u001b[32m     13\u001b[39m     get_tensor_model_parallel_world_size,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     17\u001b[39m __all__ = [\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBasevLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPackedvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRowvLLMParameter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommunication_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/communication_op.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtensor_model_parallel_all_reduce\u001b[39m(input_: torch.Tensor) -> torch.Tensor:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/distributed/parallel_state.py:250\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.ops.symm_mem.fused_scaled_matmul_reduce_scatter(\n\u001b[32m    233\u001b[39m         A,\n\u001b[32m    234\u001b[39m         B,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         use_fast_accum,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m supports_custom_op():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[43mdirect_register_custom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall_reduce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_reduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_reduce_fake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     direct_register_custom_op(\n\u001b[32m    257\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mreduce_scatter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    258\u001b[39m         op_func=reduce_scatter,\n\u001b[32m    259\u001b[39m         fake_impl=reduce_scatter_fake,\n\u001b[32m    260\u001b[39m     )\n\u001b[32m    262\u001b[39m     direct_register_custom_op(\n\u001b[32m    263\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mall_gather\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         op_func=all_gather,\n\u001b[32m    265\u001b[39m         fake_impl=all_gather_fake,\n\u001b[32m    266\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/utils/torch_utils.py:638\u001b[39m, in \u001b[36mdirect_register_custom_op\u001b[39m\u001b[34m(op_name, op_func, mutates_args, fake_impl, target_lib, dispatch_key, tags)\u001b[39m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [weak_ref_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(weak_ref_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n\u001b[32m    640\u001b[39m \u001b[38;5;66;03m# For IntermediateTensors used in pipeline parallelism\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/platforms/__init__.py:257\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    256\u001b[39m     platform_cls_qualname = resolve_current_platform_cls_qualname()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     _current_platform = \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[32m    259\u001b[39m     _init_trace = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(traceback.format_stack())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/utils/import_utils.py:122\u001b[39m, in \u001b[36mresolve_obj_by_qualname\u001b[39m\u001b[34m(qualname)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03mResolve an object by its fully-qualified class name.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m module_name, obj_name = qualname.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/vllm/platforms/cuda.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m import_pynvml\n",
      "\u001b[31mImportError\u001b[39m: /usr/local/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEiPKcS2_ib"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "model_path = \"/mnt/workspace/model/gpt-oss-20b\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    dtype = dtype,\n",
    "    max_seq_length = max_seq_length, \n",
    "    load_in_4bit = True,  \n",
    "    full_finetuning = False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78145166232e564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41001ce28aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"A dummy model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Simulate model loading.\"\"\"\n",
    "        print(\"Loading model...\")\n",
    "        # Just return a \"model\" that always answers with 0\n",
    "        return lambda problem: 0\n",
    "\n",
    "    def predict(self, problem: str):\n",
    "        # Employ lazy loading: load model on the first model.predict call\n",
    "        if self._model is None:\n",
    "            self._model = self.load()\n",
    "        return self._model(problem)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 99999, inclusive.\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values\n",
    "    id_ = id_.item(0)\n",
    "    problem_text: str = problem.item(0)\n",
    "    # Make a prediction\n",
    "    # The model is loaded on the first call\n",
    "    prediction = model.predict(problem_text)\n",
    "    return pl.DataFrame({'id': id_, 'answer': prediction})\n",
    "\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # You MUST call this within 15 minutes of the script starting. This is to\n",
    "    # ensure a \"fast fail\" in case a bug prevents the inference server from starting.\n",
    "    # Do anything that might take a long time (like model loading) in the predict\n",
    "    # function, which has no time limit.\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
