{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "技术链：\n",
    "- model gpt-oss\n",
    "- train unsloth\n",
    "- format openai-harmony\n",
    "- inference vllm\n",
    "- chat openai"
   ],
   "id": "b68226299cd8f723"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"A dummy model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Simulate model loading.\"\"\"\n",
    "        print(\"Loading model...\")\n",
    "        # Just return a \"model\" that always answers with 0\n",
    "        return lambda problem: 0\n",
    "\n",
    "    def predict(self, problem: str):\n",
    "        # Employ lazy loading: load model on the first model.predict call\n",
    "        if self._model is None:\n",
    "            self._model = self.load()\n",
    "        return self._model(problem)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 99999, inclusive.\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values\n",
    "    id_ = id_.item(0)\n",
    "    problem_text: str = problem.item(0)\n",
    "    # Make a prediction\n",
    "    # The model is loaded on the first call\n",
    "    prediction = model.predict(problem_text)\n",
    "    return pl.DataFrame({'id': id_, 'answer': prediction})\n",
    "\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # You MUST call this within 15 minutes of the script starting. This is to\n",
    "    # ensure a \"fast fail\" in case a bug prevents the inference server from starting.\n",
    "    # Do anything that might take a long time (like model loading) in the predict\n",
    "    # function, which has no time limit.\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )\n"
   ],
   "id": "cb41001ce28aad6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
