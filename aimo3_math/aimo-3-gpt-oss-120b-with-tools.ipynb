{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(input_archive, temp_dir):\n",
    "\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable, \n",
    "        '-m', \n",
    "        'pip', \n",
    "        'install', \n",
    "        '--no-index', \n",
    "        '--find-links', \n",
    "        f'{temp_dir}/wheels', \n",
    "        'unsloth', \n",
    "        'trl', \n",
    "        'vllm', \n",
    "        'openai_harmony'\n",
    "    ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "set_env(\n",
    "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName, \n",
    "    load_harmony_encoding, \n",
    "    SystemContent, \n",
    "    ReasoningEffort, \n",
    "    ToolNamespaceConfig, \n",
    "    Author, \n",
    "    Message, \n",
    "    Role, \n",
    "    TextContent, \n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "    system_prompt = (\n",
    "        'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
    "        'The final answer must be a non-negative integer between 0 and 99999. '\n",
    "        'You must place the final integer answer inside \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code. '\n",
    "        'The environment is a stateful Jupyter notebook. '\n",
    "        'You must use print() to output results.'\n",
    "    )\n",
    "    \n",
    "    preference_prompt = (\n",
    "        'Use `math`, `numpy` and `sympy` to solve the problem.'\n",
    "    )\n",
    "\n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    \n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "\n",
    "    high_problem_timeout = 900\n",
    "    base_problem_timeout = 300\n",
    "\n",
    "    notebook_limit = 17400\n",
    "    server_timeout = 180\n",
    "\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 10\n",
    "    sandbox_timeout = 5\n",
    "\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    search_tokens = 1024\n",
    "    buffer_tokens = 512\n",
    "    batch_size = 256\n",
    "    early_stop = 4\n",
    "    attempts = 8\n",
    "    workers = 16\n",
    "    turns = 128\n",
    "    seed = 42\n",
    "\n",
    "    gpu_memory_utilization = 0.96\n",
    "    temperature = 1.0\n",
    "    min_p = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Template:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(\n",
    "        self, \n",
    "        system_prompt: str, \n",
    "        user_prompt: str, \n",
    "        tool_config: ToolNamespaceConfig\n",
    "    ) -> list[Message]:\n",
    "\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "\n",
    "        return [system_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Sandbox:\n",
    "\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import numpy as np\\n'\n",
    "        )\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "\n",
    "        clean_lines = []\n",
    "\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "\n",
    "            clean_lines.append(clean_frame)\n",
    "\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
    "\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        \n",
    "        msg_id = client.execute(\n",
    "            code, \n",
    "            store_history=True, \n",
    "            allow_stdin=False, \n",
    "            stop_on_error=False\n",
    "        )\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.execute('%reset -f')\n",
    "        self.execute('import gc; gc.collect()')\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import numpy as np\\n'\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Tool:\n",
    "\n",
    "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
    "\n",
    "        self._local_jupyter_timeout = local_jupyter_timeout\n",
    "        self._tool_prompt = tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "        \n",
    "        self._owns_session = sandbox is None\n",
    "        \n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "\n",
    "        lines = code.strip().split('\\n')\n",
    "\n",
    "        if not lines:\n",
    "            return code\n",
    "\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        if 'print' in last_line or 'import' in last_line:\n",
    "            return code\n",
    "\n",
    "        if not last_line:\n",
    "            return code\n",
    "\n",
    "        if last_line.startswith('#'):\n",
    "            return code\n",
    "\n",
    "        lines[-1] = 'print(' + last_line + ')'\n",
    "\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "\n",
    "        return self._tool_prompt\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "\n",
    "        return ToolNamespaceConfig(\n",
    "            name='python', \n",
    "            description=self.instruction, \n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "\n",
    "        return message\n",
    "\n",
    "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
    "\n",
    "        self._ensure_session()\n",
    "        raw_script = message.content[0].text\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "\n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "\n",
    "        return [self._make_response(output, channel=message.channel)]\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        if self._jupyter_session is not None:\n",
    "            if self._owns_session:\n",
    "                self._jupyter_session.close()\n",
    "\n",
    "            self._jupyter_session = None\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.port = port\n",
    "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.template = AIMO3Template()\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "\n",
    "        self._preload_model_weights()\n",
    "        \n",
    "        self.server_process = self._start_server()\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=self.base_url, \n",
    "            api_key=self.api_key, \n",
    "            timeout=self.cfg.session_timeout\n",
    "        )\n",
    "\n",
    "        self._wait_for_server()\n",
    "        self._initialize_kernels()\n",
    "\n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "\n",
    "    def _preload_model_weights(self) -> None:\n",
    "\n",
    "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "\n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "\n",
    "        def _read_file(path: str) -> None:\n",
    "\n",
    "            with open(path, 'rb') as file_object:\n",
    "                while file_object.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
    "\n",
    "    def _start_server(self) -> subprocess.Popen:\n",
    "\n",
    "        cmd = [\n",
    "            sys.executable, \n",
    "            '-m', \n",
    "            'vllm.entrypoints.openai.api_server', \n",
    "            '--seed', \n",
    "            str(self.cfg.seed), \n",
    "            '--model', \n",
    "            self.cfg.model_path, \n",
    "            '--served-model-name', \n",
    "            self.cfg.served_model_name, \n",
    "            '--tensor-parallel-size', \n",
    "            '1', \n",
    "            '--max-num-seqs', \n",
    "            str(self.cfg.batch_size), \n",
    "            '--gpu-memory-utilization', \n",
    "            str(self.cfg.gpu_memory_utilization), \n",
    "            '--host', \n",
    "            '0.0.0.0', \n",
    "            '--port', \n",
    "            str(self.port), \n",
    "            '--dtype', \n",
    "            self.cfg.dtype, \n",
    "            '--kv-cache-dtype', \n",
    "            self.cfg.kv_cache_dtype, \n",
    "            '--max-model-len', \n",
    "            str(self.cfg.context_tokens), \n",
    "            '--stream-interval', \n",
    "            str(self.cfg.stream_interval), \n",
    "            '--async-scheduling', \n",
    "            '--enable-prefix-caching'\n",
    "        ]\n",
    "\n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "\n",
    "        return subprocess.Popen(\n",
    "            cmd, \n",
    "            stdout=self.log_file, \n",
    "            stderr=subprocess.STDOUT, \n",
    "            start_new_session=True\n",
    "        )\n",
    "\n",
    "    def _wait_for_server(self):\n",
    "\n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            return_code = self.server_process.poll()\n",
    "\n",
    "            if return_code is not None:\n",
    "                self.log_file.flush()\n",
    "\n",
    "                with open('vllm_server.log', 'r') as log_file:\n",
    "                    logs = log_file.read()\n",
    "\n",
    "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
    "\n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
    "\n",
    "                return\n",
    "\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "\n",
    "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
    "\n",
    "    def _initialize_kernels(self) -> None:\n",
    "\n",
    "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.sandbox_pool = queue.Queue()\n",
    "\n",
    "        def _create_sandbox():\n",
    "            \n",
    "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
    "\n",
    "    def _scan_for_answer(self, text: str) -> int | None:\n",
    "\n",
    "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "\n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _process_attempt(\n",
    "        self, \n",
    "        problem: str, \n",
    "        system_prompt: str, \n",
    "        attempt_index: int, \n",
    "        stop_event: threading.Event, \n",
    "        deadline: float\n",
    "    ) -> dict:\n",
    "\n",
    "        if stop_event.is_set() or time.time() > deadline:\n",
    "            return {\n",
    "                'Attempt': attempt_index + 1, \n",
    "                'Answer': None, \n",
    "                'Python Calls': 0, \n",
    "                'Python Errors': 0, \n",
    "                'Response Length': 0\n",
    "            }\n",
    "\n",
    "        local_tool = None\n",
    "        sandbox = None\n",
    "        python_calls = 0\n",
    "        python_errors = 0\n",
    "        total_tokens = 0\n",
    "        final_answer = None\n",
    "\n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "\n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "\n",
    "            local_tool = AIMO3Tool(\n",
    "                local_jupyter_timeout=self.cfg.jupyter_timeout, \n",
    "                tool_prompt=self.cfg.tool_prompt, \n",
    "                sandbox=sandbox\n",
    "            )\n",
    "\n",
    "            encoding = self.encoding\n",
    "            messages = self.template.apply_chat_template(\n",
    "                system_prompt, \n",
    "                problem, \n",
    "                local_tool.tool_config\n",
    "            )\n",
    "\n",
    "            conversation = Conversation.from_messages(messages)\n",
    "\n",
    "            for _ in range(self.cfg.turns):\n",
    "                if stop_event.is_set() or time.time() > deadline:\n",
    "                    break\n",
    "\n",
    "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "\n",
    "                if max_tokens < self.cfg.buffer_tokens:\n",
    "                    break\n",
    "\n",
    "                stream = self.client.completions.create(\n",
    "                    model=self.cfg.served_model_name, \n",
    "                    temperature=self.cfg.temperature, \n",
    "                    max_tokens=max_tokens, \n",
    "                    prompt=prompt_ids, \n",
    "                    seed=attempt_seed, \n",
    "                    stream=True, \n",
    "                    extra_body={\n",
    "                        'min_p': self.cfg.min_p, \n",
    "                        'stop_token_ids': self.stop_token_ids, \n",
    "                        'return_token_ids': True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    token_buffer = []\n",
    "                    text_chunks = []\n",
    "\n",
    "                    for chunk in stream:\n",
    "                        if stop_event.is_set() or time.time() > deadline:\n",
    "                            break\n",
    "\n",
    "                        new_tokens = chunk.choices[0].token_ids\n",
    "                        new_text = chunk.choices[0].text\n",
    "\n",
    "                        if new_tokens:\n",
    "                            token_buffer.extend(new_tokens)\n",
    "                            total_tokens += len(new_tokens)\n",
    "                            text_chunks.append(new_text)\n",
    "\n",
    "                        if '}' in new_text:\n",
    "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                            answer = self._scan_for_answer(search_text)\n",
    "\n",
    "                            if answer is not None:\n",
    "                                final_answer = answer\n",
    "                                break\n",
    "\n",
    "                finally:\n",
    "                    stream.close()\n",
    "\n",
    "                if final_answer is not None:\n",
    "                    break\n",
    "\n",
    "                if not token_buffer:\n",
    "                    break\n",
    "\n",
    "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "                conversation.messages.extend(new_messages)\n",
    "                last_message = new_messages[-1]\n",
    "\n",
    "                if last_message.channel == 'final':\n",
    "                    answer_text = last_message.content[0].text\n",
    "                    final_answer = self._scan_for_answer(answer_text)\n",
    "                    break\n",
    "\n",
    "                if last_message.recipient == 'python':\n",
    "                    python_calls += 1\n",
    "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
    "\n",
    "                    response_text = tool_responses[0].content[0].text\n",
    "\n",
    "                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n",
    "                        python_errors += 1\n",
    "\n",
    "                    conversation.messages.extend(tool_responses)\n",
    "\n",
    "        except Exception as exc:\n",
    "            python_errors += 1\n",
    "\n",
    "        finally:\n",
    "            if local_tool is not None:\n",
    "                local_tool.close()\n",
    "\n",
    "            if sandbox is not None:\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "\n",
    "        return {\n",
    "            'Attempt': attempt_index + 1, \n",
    "            'Response Length': total_tokens, \n",
    "            'Python Calls': python_calls, \n",
    "            'Python Errors': python_errors, \n",
    "            'Answer': final_answer\n",
    "        }\n",
    "\n",
    "    def _select_answer(self, detailed_results: list) -> int:\n",
    "\n",
    "        stats = defaultdict(lambda: {'votes': 0, 'calls': 0})\n",
    "\n",
    "        for result in detailed_results:\n",
    "            answer = result['Answer']\n",
    "\n",
    "            if answer is not None:\n",
    "                stats[answer]['votes'] += 1\n",
    "                stats[answer]['calls'] += result['Python Calls']\n",
    "\n",
    "        sorted_stats = sorted(\n",
    "            stats.items(), \n",
    "            key=lambda item: (item[1]['votes'], item[1]['calls']), \n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        vote_data = []\n",
    "\n",
    "        for answer, data in sorted_stats:\n",
    "            vote_data.append((answer, data['votes'], data['calls']))\n",
    "\n",
    "        vote_dataframe = pd.DataFrame(vote_data, columns=['Answer', 'Votes', 'Calls'])\n",
    "        display(vote_dataframe)\n",
    "\n",
    "        final_answer = sorted_stats[0][0]\n",
    "        final_votes = sorted_stats[0][1]['votes']\n",
    "        final_calls = sorted_stats[0][1]['calls']\n",
    "\n",
    "        print(f'\\nFinal Result: {final_answer} | Votes: {final_votes} | Calls: {final_calls}\\n')\n",
    "\n",
    "        return final_answer\n",
    "\n",
    "    def solve_problem(self, problem: str) -> int:\n",
    "\n",
    "        print(f'\\nProblem: {problem}\\n')\n",
    "        \n",
    "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
    "\n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = self.cfg.notebook_limit - elapsed_global\n",
    "        problems_left_others = max(0, self.problems_remaining - 1)\n",
    "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
    "\n",
    "        budget = time_left - reserved_time\n",
    "        budget = min(budget, self.cfg.high_problem_timeout)\n",
    "        budget = max(budget, self.cfg.base_problem_timeout)\n",
    "\n",
    "        deadline = time.time() + budget\n",
    "\n",
    "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
    "\n",
    "        tasks = []\n",
    "\n",
    "        for attempt_index in range(self.cfg.attempts):\n",
    "            tasks.append((self.cfg.system_prompt, attempt_index))\n",
    "\n",
    "        detailed_results = []\n",
    "        valid_answers = []\n",
    "\n",
    "        stop_event = threading.Event()\n",
    "\n",
    "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
    "\n",
    "        try:\n",
    "            futures = []\n",
    "\n",
    "            for (system_prompt, attempt_index) in tasks:\n",
    "                future = executor.submit(\n",
    "                    self._process_attempt, \n",
    "                    user_input, \n",
    "                    system_prompt, \n",
    "                    attempt_index, \n",
    "                    stop_event, \n",
    "                    deadline\n",
    "                )\n",
    "\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    detailed_results.append(result)\n",
    "\n",
    "                    if result['Answer'] is not None:\n",
    "                        valid_answers.append(result['Answer'])\n",
    "\n",
    "                    counts = Counter(valid_answers).most_common(1)\n",
    "\n",
    "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
    "                        stop_event.set()\n",
    "\n",
    "                        for f in futures:\n",
    "                            f.cancel()\n",
    "\n",
    "                        break\n",
    "\n",
    "                except Exception as exc:\n",
    "                    print(f'Future failed: {exc}')\n",
    "                    continue\n",
    "\n",
    "        finally:\n",
    "            executor.shutdown(wait=False, cancel_futures=True)\n",
    "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        if detailed_results:\n",
    "            results_dataframe = pd.DataFrame(detailed_results)\n",
    "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
    "            display(results_dataframe)\n",
    "\n",
    "        if not valid_answers:\n",
    "            print('\\nResult: 0\\n')\n",
    "\n",
    "            return 0\n",
    "\n",
    "        return self._select_answer(detailed_results)\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        if hasattr(self, 'server_process'):\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "\n",
    "        if hasattr(self, 'log_file'):\n",
    "            self.log_file.close()\n",
    "\n",
    "        if hasattr(self, 'sandbox_pool'):\n",
    "            while not self.sandbox_pool.empty():\n",
    "                try:\n",
    "                    sb = self.sandbox_pool.get_nowait()\n",
    "                    sb.close()\n",
    "\n",
    "                except Exception:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "solver = AIMO3Solver(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    \n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    \n",
    "    final_answer = solver.solve_problem(question_text)\n",
    "    \n",
    "    return pl.DataFrame({'id': id_value, 'answer': final_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "    \n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/aimo3_math/test.csv',)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 289055161,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
